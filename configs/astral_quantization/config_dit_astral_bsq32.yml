log_dir: "./runs/"
save_freq: 1
log_interval: 10
save_interval: 500
device: "cuda"
epochs: 1000
batch_size: 2
batch_length: 100
max_len: 80
pretrained_model: "DiT_uvit_tat_xlsr_ema.pth"
pretrained_encoder: ""
load_only_params: False

preprocess_params:
  sr: 22050
  spect_params:
    n_fft: 1024
    win_length: 1024
    hop_length: 256
    n_mels: 80
    fmin: 0
    fmax: 8000

model_params:
  dit_type: "DiT"
  reg_loss_type: "l1"
  diffusion_type: "flow"

  timbre_shifter:
    se_db_path: "./modules/openvoice/checkpoints_v2/converter/se_db.pt"
    ckpt_path: './modules/openvoice/checkpoints_v2/converter'

  vocoder:
    type: "hifigan"

  speech_tokenizer:
    type: 'astral_quantization'
    config_path: 'configs/astral_quantization/default_32.yml'
    checkpoint_path: 'checkpoints/models--Plachta--ASTRAL-quantization/snapshots/4a2e9679f76eb03753adc8c503e3c23bb9c22f26/bsq32/bsq32_light.pth'

  style_encoder:
    dim: 192
    campplus_path: "campplus_cn_common.bin"

  length_regulator:
    channels: 384
    is_discrete: false
    in_channels: 384  # Match ASTRAL output dimensions
    content_codebook_size: 32  # Match ASTRAL 32 codebook
    sampling_ratios: [1, 1, 1, 1]
    vector_quantize: false
    n_codebooks: 1  # Single codebook for ASTRAL
    quantizer_dropout: 0.0
    f0_condition: false
    n_f0_bins: 512

  DiT:
    hidden_dim: 384
    num_heads: 6
    depth: 9
    class_dropout_prob: 0.1
    block_size: 8192
    in_channels: 80
    style_condition: true
    final_layer_type: 'mlp'
    target: 'mel'
    content_dim: 384  # Fixed: Match ASTRAL output dimensions
    content_codebook_size: 32  # Match ASTRAL 32 codebook
    content_type: 'discrete'
    f0_condition: false
    n_f0_bins: 512
    content_codebooks: 1
    is_causal: false
    long_skip_connection: false
    zero_prompt_speech_token: false
    time_as_token: true
    style_as_token: true
    uvit_skip_connection: true
    add_resblock_in_transformer: false

loss_params:
  base_lr: 0.0001
